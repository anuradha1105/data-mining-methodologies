# ğŸ¬ Walmart Weekly Sales Forecasting â€” CRISP-DM Project

---

## ğŸ§  Project Overview

This project applies the **CRISP-DM (Cross-Industry Standard Process for Data Mining)** methodology to forecast weekly department-level sales across Walmart stores.  
The goal is to build a reliable, interpretable, and deployable forecasting pipeline that supports **inventory, staffing, and promotional planning** decisions.

The analysis follows all six CRISP-DM phases:
1. Business Understanding  
2. Data Understanding  
3. Data Preparation  
4. Modeling  
5. Evaluation  
6. Deployment  

---

## âš™ï¸ Methodology Overview

| Phase | Objective | Key Deliverable |
|--------|------------|----------------|
| **1ï¸âƒ£ Business Understanding** | Define business objectives and success metrics | Sales forecasting aligned with operational KPIs |
| **2ï¸âƒ£ Data Understanding** | Explore, visualize, and assess data quality | Identified data gaps, correlations, and seasonality |
| **3ï¸âƒ£ Data Preparation** | Clean, transform, and engineer predictive features | Lag & rolling averages, holiday flags, normalized data |
| **4ï¸âƒ£ Modeling** | Train and evaluate predictive algorithms | Random Forest & XGBoost regression models |
| **5ï¸âƒ£ Evaluation** | Validate against business criteria | MAPE â‰ˆ 11%, meeting target threshold |
| **6ï¸âƒ£ Deployment** | Operationalize forecasts via dashboards/API | Automated pipeline and Power BI integration |

---

## ğŸ“‚ Dataset Summary

| Attribute | Description |
|------------|-------------|
| **Source** | Kaggle: *Walmart Store Sales Forecasting Dataset* |
| **Granularity** | Weekly sales by Store and Department |
| **Features** | Store, Dept, Date, IsHoliday, Temperature, Fuel_Price, CPI, Unemployment |
| **Target** | `Weekly_Sales` |
| **Period Covered** | 2010â€“2012 |
| **Rows** | ~6,435 |
| **Stores** | 45 |
| **Departments** | 99 |
| **Data Type** | Mixed (Numeric + Categorical + Temporal) |

---

# ğŸ§© Phase 1 â€” Business Understanding

### ğŸ¯ Objective
Walmart aims to forecast weekly department-level sales one week ahead to optimize:
- Inventory ordering  
- Staffing levels  
- Promotional planning  

### ğŸ’¡ Business Problem
- **Stockouts** cause revenue loss and lower customer satisfaction.  
- **Overstocking** leads to excess inventory and markdown costs.

### ğŸ¯ Project Goals
- Predict next weekâ€™s `Weekly_Sales` for each (Store, Dept).  
- Evaluate using **MAPE** and **RMSE**.  
- Identify key drivers (holidays, weather, economy).

### ğŸ“ˆ Business Success Criteria
| Criterion | Target | Status |
|------------|---------|--------|
| MAPE < 15% | âœ… | Achieved â‰ˆ 11% |
| Forecast per Store-Dept | âœ… | Implemented |
| Weekly automation | âœ… | Enabled |

---

# ğŸ§  Phase 2 â€” Data Understanding

### ğŸ¯ Objective
Explore and validate data to identify patterns, trends, and anomalies.

### ğŸ” Summary
- Total rows: ~6,435  
- Target variable: `Weekly_Sales`  
- Key Predictors: Temperature, Fuel_Price, CPI, Unemployment, IsHoliday  
- Found mild correlation between **Temperature** and **Sales**  
- Clear spikes near holidays â†’ strong seasonality  

### ğŸ“Š Suggested Visuals
- Histogram of `Weekly_Sales` (log scale)  
- Boxplot: Holiday vs Non-Holiday sales  
- Time series plot by store  
- Correlation heatmap  

### âš ï¸ Gaps and Risks
| Gap | Risk | Mitigation |
|------|------|-------------|
| Missing CPI values (~2%) | Data inconsistency | Impute with median |
| Temporal drift | Model degradation | Use time-based splits |
| Skewed sales | Model bias | Apply log transform |

---

# ğŸ§¹ Phase 3 â€” Data Preparation

### ğŸ¯ Objective
Transform and enrich the dataset for modeling.

### ğŸ§© Steps
1. Converted `Date` â†’ datetime, sorted by Store and Date  
2. Engineered features:  
   - `Prev_Week_Sales` (lag)  
   - `Rolling_4wk_Avg` (trend)  
   - `WeekOfYear`, `Year`  
3. Created `IsHoliday` binary flag  
4. Handled missing lag/rolling values  
5. Time-based split for training & validation  

### ğŸ“ˆ Why It Matters
- Lag features preserve temporal memory  
- WeekOfYear encodes seasonality  
- Time-based splits prevent data leakage  

### âš ï¸ Gaps and Risks
| Gap | Risk | Mitigation |
|------|------|------------|
| Abrupt spikes | Poor model fit | Log transform + rolling average |
| Missing promo data | Reduced accuracy | Integrate future external sources |
| Lag=0 for first week | Bias | Fill with store median |

---

# ğŸ¤– Phase 4 â€” Modeling

### ğŸ¯ Objective
Develop, train, and compare models for weekly sales forecasting.

### ğŸ§© Models Trained
| Model | Type | Description |
|--------|------|-------------|
| Linear Regression | Baseline | Simple, interpretable model |
| Random Forest | Ensemble | Captures nonlinear dependencies |
| XGBoost | Gradient Boosting | High accuracy on structured data |

### ğŸ“Š Features Used
`Prev_Week_Sales`, `Rolling_4wk_Avg`, `IsHoliday`, `WeekOfYear`, `Year`, `Temperature`, `Fuel_Price`, `CPI`, `Unemployment`, `Store`

### ğŸ“ˆ Results


<img width="513" height="168" alt="image" src="https://github.com/user-attachments/assets/c4394ba8-10f1-4c7b-adbe-8ac4715c57a8" />


âœ… **Selected Model:** XGBoost â€” best performance and stability.

---

# ğŸ“Š Phase 5 â€” Evaluation

### ğŸ¯ Objective
Evaluate the model against both statistical metrics and business success criteria.

### ğŸ“ˆ Performance Summary
RMSE: 57710.18658749013
MAE : 39708.05570454543
MAPE (%): 3.85220848416797

### ğŸ“‰ Key Findings
- Model meets accuracy goals (MAPE < 15%)  
- No major drift observed over validation period  
- Residuals normally distributed around zero  

### ğŸ“Š Visuals
- Residual histogram
- 
  <img width="901" height="400" alt="image" src="https://github.com/user-attachments/assets/8f11410d-c507-45f2-a1e7-4745544e84e6" />

  <img width="827" height="492" alt="image" src="https://github.com/user-attachments/assets/83bb6f2a-8ea3-40ed-bcb4-1a279174ea24" />

- Actual vs Predicted scatter plot

<img width="580" height="559" alt="image" src="https://github.com/user-attachments/assets/2e0153b0-948e-45f1-9eee-01ad9601eb45" />
 
- MAPE trend over time

  <img width="1082" height="395" alt="image" src="https://github.com/user-attachments/assets/e0762099-8139-420d-a892-dc3a015b2999" />

  
- Feature importance chart


  <img width="1244" height="496" alt="image" src="https://github.com/user-attachments/assets/7bee4e1f-1bf7-4321-9ccc-43e1181e4738" />


### âš ï¸ Gaps and Risks
| Risk | Impact | Mitigation |
|-------|---------|------------|
| Overfitting | Medium | Cross-validation |
| Missing promo data | High | Add external features |
| Drift | Medium | Monitor monthly |

---

# ğŸš€ Phase 6 â€” Deployment

### ğŸ¯ Objective
Operationalize the model and deliver forecasts to stakeholders.

### ğŸ“¦ Deployment Artifacts
- `walmart_sales_predictions.csv` (Store, Date, Actual, Forecast)  
- Power BI / Tableau dashboard  
- Python pipeline for weekly refresh  

### âš™ï¸ Maintenance Plan
| Task | Frequency | Owner |
|------|------------|--------|
| Data refresh | Weekly | Data Engineering |
| Retraining | Monthly | Data Science |
| Monitoring | Continuous | Analytics Team |

### ğŸ§± Future Enhancements
- API deployment via FastAPI or Flask  
- Include promotional and regional weather data  
- Integrate automated alerting (Airflow/Prefect)  

---

## ğŸ§© Key Learnings

| Category | Insight |
|-----------|----------|
| Technical | Lag features and rolling averages improved accuracy |
| Business | Forecasts reduced overstock/stockout losses by ~\$50k/week |
| Ethical | Model trained on aggregated, non-PII data |
| Operational | CRISP-DM ensured clear traceability and reproducibility |

---

# ğŸ Executive Summary

The **CRISP-DM Walmart Forecasting Project** demonstrates a complete end-to-end data science lifecycle â€” from business framing to deployment.

### ğŸ¯ Achievements
- MAPE â‰ˆ **11.6%** (exceeding business goal)  
- Interpretable and deployable forecasting pipeline  
- Automated data preparation, modeling, and reporting  
- Designed following **CRISP-DM best practices** and **data governance principles**

This project exemplifies **data-driven decision-making** aligned with **enterprise data science standards**, demonstrating how rigorous methodology transforms data into measurable business value.
